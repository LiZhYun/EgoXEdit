"""
Enhanced Wan2.1-VACE-1.3B with VACE-E Support

This script demonstrates enhanced video generation using the WanVideoPipeline
with VACE-E (Video Animation and Control Engine - Enhanced) capabilities.

VACE-E extends the original VACE framework with task-embodiment fusion for
robot manipulation video generation:

ğŸ¯ Key Enhancements:
1. Task Feature Processing:
   - Text descriptions of robot tasks
   - Dual-hand motion sequences (left/right wrist poses + gripper states)
   - Object trajectory sequences with type embeddings

2. Embodiment Feature Processing:
   - CLIP-encoded end-effector images showing robot gripper
   - Provides robot-specific visual context

3. Multi-modal Fusion:
   - Independent weighted fusion of task and embodiment features
   - Cross-attention alignment between modalities
   - Reduced correlation for better learning

4. Integration:
   - Works alongside standard VACE (not replacing)
   - Automatic DiT weight initialization
   - Memory-efficient with full VRAM management

ğŸ“ Data Sources:
- HDF5 files: Generated by /home//Codes/human-policy/data/save_object_trajectory.py
- Dataset path: /home//Codes/DataSets/small_test
- Expected data: dual-hand motion sequences (20D), object trajectories, end-effector images

ğŸ¤– Dual-Hand Motion Format (NEW):
- 20D input: [left_wrist(9), right_wrist(9), left_gripper(1), right_gripper(1)]
- Supports coordinated left/right hand manipulation
- Automatic handling of single-hand data (right hand active, left hand zeros)
- Backward compatibility with legacy 10D single-hand format

ğŸ”§ Compared to original Wan2.1-VACE-1.3B.py:
- Uses enhanced pipeline (wan_video_new_E vs wan_video_new)
- Loads robot demonstration data from HDF5 files with dual-hand support
- Processes task and embodiment features
- Applies both VACE and VACE-E conditioning simultaneously
- Provides detailed feature usage summary

ğŸš€ Output:
Enhanced robot manipulation videos with:
- Traditional VACE video editing capabilities
- Task-aware generation from robot demonstrations
- Dual-hand embodiment synthesis using coordinated left/right hands
- Embodiment-aware synthesis using end-effector context
- Improved temporal consistency and robot-specific content
"""

import torch
from PIL import Image
import numpy as np
import h5py
import json
import argparse
from diffsynth import save_video, VideoData, load_state_dict
from diffsynth.pipelines.wan_video_new_E import WanVideoPipeline, ModelConfig
from modelscope import dataset_snapshot_download
import os
import tempfile
import shutil
import glob


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Enhanced Wan2.1-VACE-1.3B with VACE-E Support")
    
    # Model and weights
    parser.add_argument("--state_dict_path", type=str, 
                       default="/scratch/work/liz23/h2rego_video_generation/models/train/Wan2.1-VACE-E-1.3B_full_no_club/epoch-2.safetensors",
                       help="Path to the state dict file to load")
    
    # Data paths
    parser.add_argument("--dataset_path", type=str, 
                       default="/scratch/work/liz23/DataSets/picking",
                       help="Path to the dataset folder containing robot demonstration data")
    
    parser.add_argument("--end_effector_image", type=str,
                       default="/scratch/work/liz23/DataSets/PH2D_videos/405-pick_on_color_pad_right_far_far-2025_01_13-19_29_04/episode_7/episode_7_hands_only.jpg",
                       help="Path to the end-effector image to use")
    
    # Generation parameters
    parser.add_argument("--vace_e_scale", type=float, default=1.0,
                       help="VACE-E conditioning strength (default: 1.0)")
    
    # Output configuration
    parser.add_argument("--output_dir", type=str, default="videos_no_club",
                       help="Output directory name under ./output/ (default: videos_no_club)")
    
    return parser.parse_args()


def load_task_metadata(metadata_path="/scratch/work/liz23/h2rego_data_preprocess/data/ph2d_metadata.json"):
    """
    Load task metadata from JSON file.
    
    Args:
        metadata_path: Path to PH2D metadata JSON file
        
    Returns:
        dict: Metadata dictionary or None if loading fails
    """
    try:
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        print(f"âœ… Loaded metadata from: {metadata_path}")
        return metadata
    except Exception as e:
        print(f"âš ï¸ Failed to load metadata from {metadata_path}: {e}")
        return None


def generate_task_prompt(task_name, metadata):
    """
    Generate robot task prompt based on metadata.
    
    Args:
        task_name: Name of the task (e.g., "109-grasping-mtn-human_2024-11-13_14-40-07")
        metadata: Metadata dictionary
        
    Returns:
        str: Generated task prompt
    """
    if not metadata or "per_task_attributes" not in metadata:
        # Fallback prompt
        return "Robot manipulation task with hand movements and object interactions."
    
    task_attrs = metadata["per_task_attributes"].get(task_name)
    if not task_attrs:
        print(f"âš ï¸ Task {task_name} not found in metadata, using fallback prompt")
        return "Robot manipulation task with hand movements and object interactions."
    
    # Extract task information
    task_type = task_attrs.get("task_type", "manipulation")
    objects = task_attrs.get("objects", "objects")
    left_hand = task_attrs.get("left_hand", False)
    right_hand = task_attrs.get("right_hand", False)
    
    print(f"Task info - Type: {task_type}, Objects: {objects}, Left: {left_hand}, Right: {right_hand}")
    
    # Generate prompt based on hand usage
    if left_hand and right_hand:
        if task_type == "pouring":
            prompt = f"Pouring, left hand cup, right hand bottle"
        else:
            prompt = f"Both hands {task_type} {objects}"
    elif right_hand:
        prompt = f"Right hand {task_type} {objects}"
    elif left_hand:
        prompt = f"Left hand {task_type} {objects}"
    else:
        prompt = f"{task_type.capitalize()} {objects}"
    
    return prompt


def load_robot_data_from_hdf5(dataset_path="/scratch/work/liz23/DataSets/PH2D_videos"):
    """
    Load robot demonstration data from HDF5 files for VACE-E processing.
    
    Expected folder structure:
    dataset_path/
    â””â”€â”€ task_folder/ (e.g., 104-lars-grasping_2024-11-08_15-23-40)
        â”œâ”€â”€ episode_0/
        â”‚   â”œâ”€â”€ episode_0_hand_trajectories.hdf5
        â”‚   â”œâ”€â”€ episode_0_object_trajectories.hdf5
        â”‚   â”œâ”€â”€ episode_0_hands_masked.mp4
        â”‚   â””â”€â”€ episode_0_hands_mask.mp4
        â”œâ”€â”€ episode_1/
        â”‚   â”œâ”€â”€ episode_1_hand_trajectories.hdf5
        â”‚   â”œâ”€â”€ episode_1_object_trajectories.hdf5
        â”‚   â”œâ”€â”€ episode_1_hands_masked.mp4
        â”‚   â””â”€â”€ episode_1_hands_mask.mp4
        â””â”€â”€ ...
    
    Args:
        dataset_path: Path to dataset folder containing task folders
        
    Returns:
        list: List of episode data dictionaries, each containing:
              - task_name, episode_name, hand_motion_sequence (20D dual-hand format), 
                object_trajectory_sequence, object_ids, vace_video_path, vace_mask_path, 
                end_effector_image_path
              
        hand_motion_sequence format: [batch, seq_len, 20] where:
              - Dims 0-8:   Left wrist pose (3D position + 6D rotation)
              - Dims 9-17:  Right wrist pose (3D position + 6D rotation)
              - Dim 18:     Left gripper state (0=closed, 1=open)
              - Dim 19:     Right gripper state (0=closed, 1=open)
    """
    print(f"Loading robot data from: {dataset_path}")
    
    # Find all task folders
    task_folders = [d for d in os.listdir(dataset_path) 
                   if os.path.isdir(os.path.join(dataset_path, d)) and not d.startswith('.')]
    
    if not task_folders:
        print(f"âš ï¸ No task folders found in {dataset_path}")
        return []
    
    print(f"Found {len(task_folders)} task folders: {task_folders}")
    
    all_episodes = []
    
    for task_folder in task_folders:
        if not task_folder.startswith("1"):
            print(f"Skipping task {task_folder} (does not start with '1')")
            continue
        task_path = os.path.join(dataset_path, task_folder)
        print(f"\nProcessing task: {task_folder}")
        
        # Find all episode folders in this task
        episode_folders = [d for d in os.listdir(task_path) 
                          if os.path.isdir(os.path.join(task_path, d)) and d.startswith('episode_')]
        episode_folders.sort()  # Sort to process in order
        
        if not episode_folders:
            print(f"  âš ï¸ No episode folders found in {task_path}")
            continue
        
        print(f"  Found {len(episode_folders)} episodes: {episode_folders}")
        
        for episode_folder in episode_folders:
            episode_path = os.path.join(task_path, episode_folder)
            episode_name = episode_folder
            
            print(f"    Processing episode: {episode_name}")
            
            # Expected file paths
            hand_hdf5_path = os.path.join(episode_path, f"{episode_name}_hand_trajectories.hdf5")
            object_hdf5_path = os.path.join(episode_path, f"{episode_name}_object_trajectories.hdf5")
            vace_video_path = os.path.join(episode_path, f"{episode_name}_hands_masked.mp4")
            vace_mask_path = os.path.join(episode_path, f"{episode_name}_hands_mask.mp4")
            
            episode_data = {
                'task_name': task_folder,
                'episode_name': episode_name,
                'episode_path': episode_path,
                'hand_motion_sequence': None,
                'object_trajectory_sequence': None,
                'object_ids': None,
                'vace_video_path': vace_video_path if os.path.exists(vace_video_path) else None,
                'vace_mask_path': vace_mask_path if os.path.exists(vace_mask_path) else None,
                'end_effector_image_path': None
            }
            
            # Load hand motion data
            if os.path.exists(hand_hdf5_path):
                try:
                    print(f"      Loading hand trajectories from: {hand_hdf5_path}")
                    with h5py.File(hand_hdf5_path, 'r') as f:
                        print(f"      Hand HDF5 keys: {list(f.keys())}")
                        
                        # Load wrist poses for both hands (dual-hand format)
                        if 'left_wrist' in f and 'right_wrist' in f:
                            # Load left hand data
                            left_positions = f['left_wrist/positions'][:]     # [frames, 3]
                            left_rotations = f['left_wrist/rotations_6d'][:]  # [frames, 6]
                            
                            # Load right hand data  
                            right_positions = f['right_wrist/positions'][:]     # [frames, 3]
                            right_rotations = f['right_wrist/rotations_6d'][:]  # [frames, 6]
                            
                            print(f"      âœ“ Left hand - Position: {left_positions.shape}, Rotation: {left_rotations.shape}")
                            print(f"      âœ“ Right hand - Position: {right_positions.shape}, Rotation: {right_rotations.shape}")
                            
                            # # Extend 6D rotations to 9D by adding zeros (standard practice)
                            # if left_rotations.shape[1] == 6:
                            #     left_padding = np.zeros((left_rotations.shape[0], 3))
                            #     left_rotations_9d = np.concatenate([left_rotations, left_padding], axis=1)
                            # else:
                            #     left_rotations_9d = left_rotations
                                
                            # if right_rotations.shape[1] == 6:
                            #     right_padding = np.zeros((right_rotations.shape[0], 3))
                            #     right_rotations_9d = np.concatenate([right_rotations, right_padding], axis=1)
                            # else:
                            #     right_rotations_9d = right_rotations
                            
                            # Load gripper states for both hands
                            left_gripper_states = np.zeros(len(left_positions))  # Default: closed
                            right_gripper_states = np.zeros(len(right_positions))  # Default: closed
                            
                            if 'left_hand_states' in f:
                                left_gripper_states = f['left_hand_states'][:]
                                # Convert to binary: closed=0, open=1
                                left_gripper_states = (left_gripper_states == 1).astype(float)
                                print(f"      âœ“ Left gripper states loaded: unique values {np.unique(left_gripper_states)}")
                            else:
                                print(f"      âš ï¸ No left_hand_states found, using default (closed)")
                                
                            if 'right_hand_states' in f:
                                right_gripper_states = f['right_hand_states'][:]
                                # Convert to binary: closed=0, open=1  
                                right_gripper_states = (right_gripper_states == 1).astype(float)
                                print(f"      âœ“ Right gripper states loaded: unique values {np.unique(right_gripper_states)}")
                            else:
                                print(f"      âš ï¸ No right_hand_states found, using default (closed)")
                            
                            # Create dual-hand 20D format: [left_wrist(9), right_wrist(9), left_gripper(1), right_gripper(1)]
                            left_wrist_9d = np.concatenate([left_positions, left_rotations], axis=1)   # [frames, 9]
                            right_wrist_9d = np.concatenate([right_positions, right_rotations], axis=1) # [frames, 9]
                            
                            hand_motion_data = np.concatenate([
                                left_wrist_9d,                           # [frames, 9] - left wrist pose
                                right_wrist_9d,                          # [frames, 9] - right wrist pose  
                                left_gripper_states.reshape(-1, 1),     # [frames, 1] - left gripper state
                                right_gripper_states.reshape(-1, 1)     # [frames, 1] - right gripper state
                            ], axis=1)  # Result: [frames, 20]
                            
                            episode_data['hand_motion_sequence'] = torch.from_numpy(hand_motion_data).to(torch.bfloat16)
                            print(f"      âœ… Dual-hand motion shape: {episode_data['hand_motion_sequence'].shape} (20D format)")
                            print(f"         - Dtype: {episode_data['hand_motion_sequence'].dtype}")
                            print(f"         - Left wrist (dims 0-8): position + rotation")
                            print(f"         - Right wrist (dims 9-17): position + rotation") 
                            print(f"         - Left gripper (dim 18): {np.unique(left_gripper_states)}")
                            print(f"         - Right gripper (dim 19): {np.unique(right_gripper_states)}")
                            
                        elif 'right_wrist' in f:
                            # Only right hand data available - create dual-hand with left hand as zeros
                            print(f"      âš ï¸ Only right hand data found, creating dual-hand with left=zeros")
                            
                            right_positions = f['right_wrist/positions'][:]     # [frames, 3]
                            right_rotations = f['right_wrist/rotations_6d'][:]  # [frames, 6]
                            
                            # Extend 6D rotations to 9D
                            if right_rotations.shape[1] == 6:
                                right_padding = np.zeros((right_rotations.shape[0], 3))
                                right_rotations_9d = np.concatenate([right_rotations, right_padding], axis=1)
                            else:
                                right_rotations_9d = right_rotations
                            
                            # Load right gripper states
                            if 'right_hand_states' in f:
                                right_gripper_states = f['right_hand_states'][:]
                                right_gripper_states = (right_gripper_states == 1).astype(float)
                            else:
                                right_gripper_states = np.ones(len(right_positions)) * 0.5  # Neutral
                            
                            # Create dual-hand with left hand as zeros
                            left_wrist_zeros = np.zeros((len(right_positions), 9))    # Left wrist: all zeros
                            left_gripper_zeros = np.zeros(len(right_positions))       # Left gripper: closed
                            right_wrist_9d = np.concatenate([right_positions, right_rotations_9d], axis=1)
                            
                            hand_motion_data = np.concatenate([
                                left_wrist_zeros,                        # [frames, 9] - left wrist (zeros)
                                right_wrist_9d,                          # [frames, 9] - right wrist (active)
                                left_gripper_zeros.reshape(-1, 1),      # [frames, 1] - left gripper (closed)
                                right_gripper_states.reshape(-1, 1)     # [frames, 1] - right gripper (active)
                            ], axis=1)  # Result: [frames, 20]
                            
                            episode_data['hand_motion_sequence'] = torch.from_numpy(hand_motion_data).to(torch.bfloat16)
                            print(f"      âœ… Singleâ†’Dual hand motion shape: {episode_data['hand_motion_sequence'].shape} (20D format)")
                            print(f"         - Dtype: {episode_data['hand_motion_sequence'].dtype}")
                            print(f"         - Left hand: inactive (zeros)")
                            print(f"         - Right hand: active (actual data)")
                        
                        else:
                            print(f"      âš ï¸ No valid wrist data found in HDF5 file")
                        
                except Exception as e:
                    print(f"      âš ï¸ Error loading hand trajectories: {e}")
            
            # Load object trajectory data
            if os.path.exists(object_hdf5_path):
                try:
                    print(f"      Loading object trajectories from: {object_hdf5_path}")
                    with h5py.File(object_hdf5_path, 'r') as f:
                        print(f"      Object HDF5 keys: {list(f.keys())}")
                        
                        # Get all object groups
                        object_groups = [key for key in f.keys() if key.startswith('object_')]
                        
                        if object_groups:
                            all_object_trajectories = []
                            object_ids = []
                            
                            for obj_group_name in sorted(object_groups):
                                obj_group = f[obj_group_name]
                                obj_id = obj_group.attrs.get('object_id', len(object_ids))
                                
                                if 'positions' in obj_group and 'rotations_6d' in obj_group:
                                    positions = obj_group['positions'][:]  # [frames, 3]
                                    rotations_6d = obj_group['rotations_6d'][:]  # [frames, 6]
                                    
                                    # # Extend 6D rotation to 9D for consistency
                                    # if rotations_6d.shape[1] == 6:
                                    #     padding = np.zeros((rotations_6d.shape[0], 3))
                                    #     rotations_9d = np.concatenate([rotations_6d, padding], axis=1)
                                    # else:
                                    #     rotations_9d = rotations_6d
                                    
                                    # Combine position + rotation = 9D per object
                                    obj_trajectory = np.concatenate([positions, rotations_6d], axis=1)  # [frames, 9]
                                    all_object_trajectories.append(obj_trajectory)
                                    object_ids.append(obj_id)
                                    
                                    print(f"        âœ“ Object {obj_id} trajectory shape: {obj_trajectory.shape}")
                            
                            if all_object_trajectories:
                                # Stack all object trajectories: [frames, num_objects, 9]
                                object_trajectory_data = np.stack(all_object_trajectories, axis=1)
                                episode_data['object_trajectory_sequence'] = torch.from_numpy(object_trajectory_data).to(torch.bfloat16)
                                episode_data['object_ids'] = torch.tensor(object_ids, dtype=torch.long)
                                
                                print(f"      âœ“ Object trajectory shape: {episode_data['object_trajectory_sequence'].shape}")
                                print(f"      âœ“ Object trajectory dtype: {episode_data['object_trajectory_sequence'].dtype}")
                                print(f"      âœ“ Object IDs: {episode_data['object_ids']}")
                            else:
                                print(f"      âš ï¸ No valid object trajectories found")
                        
                except Exception as e:
                    print(f"      âš ï¸ Error loading object trajectories: {e}")
            
            # Find end-effector image (look for robot/gripper related images)
            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
            for ext in image_extensions:
                image_files = glob.glob(os.path.join(episode_path, ext))
                if image_files:
                    # Look for gripper/end-effector related images
                    for img_file in image_files:
                        if any(keyword in img_file.lower() for keyword in ['gripper', 'end_effector', 'robot', 'hand']):
                            episode_data['end_effector_image_path'] = img_file
                            break
                    if episode_data['end_effector_image_path'] is None:
                        episode_data['end_effector_image_path'] = image_files[0]  # Use first available image
                    break
            
            # Check video files
            if episode_data['vace_video_path']:
                print(f"      âœ“ VACE video: {os.path.basename(episode_data['vace_video_path'])}")
            else:
                print(f"      âš ï¸ VACE video not found: {episode_name}_hands_masked.mp4")
                
            if episode_data['vace_mask_path']:
                print(f"      âœ“ VACE mask: {os.path.basename(episode_data['vace_mask_path'])}")
            else:
                print(f"      âš ï¸ VACE mask not found: {episode_name}_hands_mask.mp4")
            
            if episode_data['end_effector_image_path']:
                print(f"      âœ“ End-effector image: {os.path.basename(episode_data['end_effector_image_path'])}")
            
            all_episodes.append(episode_data)
    print(f"\nâœ… Loaded {len(all_episodes)} episodes from {len(task_folders)} tasks")
    return all_episodes


def main():
    """Main function to run the enhanced video generation."""
    args = parse_args()
    
    # Load WanVideoPipeline with VACE-E support enabled
    pipe = WanVideoPipeline.from_pretrained(
        torch_dtype=torch.bfloat16,
        device="cuda",
        model_configs=[
            ModelConfig(model_id="Wan-AI/Wan2.1-VACE-1.3B", origin_file_pattern="diffusion_pytorch_model*.safetensors", offload_device="cpu"),
            ModelConfig(model_id="Wan-AI/Wan2.1-VACE-1.3B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", offload_device="cpu"),
            ModelConfig(model_id="Wan-AI/Wan2.1-VACE-1.3B", origin_file_pattern="Wan2.1_VAE.pth", offload_device="cpu"),
            ModelConfig(model_id="Wan-AI/Wan2.1-I2V-14B-480P", origin_file_pattern="models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", offload_device="cpu")
        ],
        # VACE-E configuration
        enable_vace_e=True,
        vace_e_layers=(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28),
        vace_e_task_processing=True,
    )

    state_dict = load_state_dict(args.state_dict_path)
    pipe.vace_e.load_state_dict(state_dict)

    pipe.enable_vram_management()

    # Load robot demonstration data for VACE-E
    print("\n=== Loading Robot Demonstration Data for VACE-E ===")
    robot_episodes = load_robot_data_from_hdf5(args.dataset_path)

    # Load task metadata for prompt generation
    print("\n=== Loading Task Metadata ===")
    task_metadata = load_task_metadata("/scratch/work/liz23/h2rego_data_preprocess/data/ph2d_metadata.json")

    if not robot_episodes:
        print("âš ï¸ No robot episodes loaded, continuing with standard VACE generation...")
        # Create a dummy episode for fallback
        robot_episodes = [{
            'task_name': 'fallback',
            'episode_name': 'fallback',
            'hand_motion_sequence': None,
            'object_trajectory_sequence': None,
            'object_ids': None,
            'vace_video_path': None,
            'vace_mask_path': None,
            'end_effector_image_path': None
        }]

    print(f"\nğŸ¬ Processing {len(robot_episodes)} episodes...")

    # Process each episode
    for episode_idx, episode_data in enumerate(robot_episodes):
        if not episode_data['task_name'].startswith("1"):
            continue
        print(f"\n{'='*60}")
        print(f"Processing Episode {episode_idx + 1}/{len(robot_episodes)}")
        print(f"Task: {episode_data['task_name']}")
        print(f"Episode: {episode_data['episode_name']}")
        print(f"{'='*60}")

        # Prepare VACE-E features for this episode
        vace_e_text_features = None
        vace_e_hand_motion_sequence = None
        vace_e_object_trajectory_sequence = None
        vace_e_object_ids = None
        vace_e_embodiment_image_features = None
        vace_e_text_mask = None
        vace_e_motion_mask = None
        vace_e_trajectory_mask = None

        print("\n=== Preparing VACE-E Features ===")
        
        # 1. Prepare text features (encode robot task description)
        robot_task_prompt = generate_task_prompt(episode_data['task_name'], task_metadata)
        print(f"Encoding robot task: {robot_task_prompt}")
        vace_e_text_features = pipe.prompter.encode_prompt(robot_task_prompt, device=pipe.device)
        vace_e_text_mask = torch.ones(1, vace_e_text_features.shape[1], device=pipe.device).bool()
        print(f"âœ“ Text features shape: {vace_e_text_features.shape}")
        
        # 2. Prepare hand motion sequence
        if episode_data['hand_motion_sequence'] is not None:
            vace_e_hand_motion_sequence = episode_data['hand_motion_sequence'].unsqueeze(0).to(pipe.device)
            vace_e_motion_mask = torch.ones(1, vace_e_hand_motion_sequence.shape[1], device=pipe.device).bool()
            print(f"âœ“ Hand motion sequence shape: {vace_e_hand_motion_sequence.shape}")
        
        # 3. Prepare object trajectory sequence
        if episode_data['object_trajectory_sequence'] is not None:
            vace_e_object_trajectory_sequence = episode_data['object_trajectory_sequence'].unsqueeze(0).to(pipe.device)
            # Fix: object_ids should be 2D [batch_size, num_objects] not 1D
            vace_e_object_ids = episode_data['object_ids'].unsqueeze(0).to(pipe.device)  # Add batch dimension
            vace_e_trajectory_mask = torch.ones(1, vace_e_object_trajectory_sequence.shape[1], vace_e_object_trajectory_sequence.shape[2], device=pipe.device).bool()
            print(f"âœ“ Object trajectory shape: {vace_e_object_trajectory_sequence.shape}")
            print(f"âœ“ Object IDs shape: {vace_e_object_ids.shape} (should be [batch_size, num_objects])")
            print(f"âœ“ Object IDs: {vace_e_object_ids}")

        episode_data['end_effector_image_path'] = args.end_effector_image
        # episode_data['end_effector_image_path'] = "/scratch/work/liz23/DataSets/PH2D_videos/303-grasp_coke_random-2024_12_12-19_13_53/episode_5/episode_5_hands_only.jpg"
        # episode_data['end_effector_image_path'] = "/scratch/work/liz23/DataSets/PH2D_videos/502-pouring_random-2025_01_10-20_21_26/episode_0/episode_0_hands_only.jpg"
        # 4. Prepare embodiment features (end-effector image)
        if episode_data['end_effector_image_path']:
            print(f"Processing end-effector image: {episode_data['end_effector_image_path']}")
            try:
                end_effector_image = Image.open(episode_data['end_effector_image_path']).resize((832, 480))
                end_effector_image = pipe.preprocess_image(end_effector_image.resize((832, 480))).to(pipe.device)
                # Encode with CLIP image encoder
                vace_e_embodiment_image_features = pipe.image_encoder.encode_image([end_effector_image])
                print(f"âœ“ Embodiment image features shape: {vace_e_embodiment_image_features.shape}")
            except Exception as e:
                print(f"âš ï¸ Failed to process end-effector image: {e}")
        
        print(f"\nâœ… VACE-E features prepared for {episode_data['episode_name']}!")
        print(f"   Text features: {vace_e_text_features.shape if vace_e_text_features is not None else 'None'}")
        print(f"   Hand motion: {vace_e_hand_motion_sequence.shape if vace_e_hand_motion_sequence is not None else 'None'}")
        print(f"   Object trajectory: {vace_e_object_trajectory_sequence.shape if vace_e_object_trajectory_sequence is not None else 'None'}")
        print(f"   Embodiment image: {vace_e_embodiment_image_features.shape if vace_e_embodiment_image_features is not None else 'None'}")

        # Prepare VACE video inputs for this episode
        print("\n=== Preparing VACE Video Inputs ===")
        
        # Load episode-specific VACE videos
        if episode_data['vace_video_path'] and episode_data['vace_mask_path']:
            print(f"Loading VACE video: {episode_data['vace_video_path']}")
            print(f"Loading VACE mask: {episode_data['vace_mask_path']}")
            
            vace_video = VideoData(episode_data['vace_video_path'], height=480, width=832)
            vace_video_mask = VideoData(episode_data['vace_mask_path'], height=480, width=832)
            
            # Reduce frame count to 81 for VRAM efficiency
            original_frames = len(vace_video)
            target_frames = 81

            if original_frames > target_frames:
                print(f"Reducing video from {original_frames} frames to {target_frames} frames")
                
                # Sample frames evenly from the video
                step = original_frames // target_frames
                sampled_indices = [i * step for i in range(target_frames)]
                
                # Extract sampled frames from both video and mask
                print("Extracting sampled frames...")
                vace_frames = [vace_video[i] for i in sampled_indices]
                mask_frames = [vace_video_mask[i] for i in sampled_indices]
                
                # Create temporary folders for sampled frames
                temp_dir = tempfile.mkdtemp()
                vace_temp_folder = os.path.join(temp_dir, "vace_frames")
                mask_temp_folder = os.path.join(temp_dir, "mask_frames")
                os.makedirs(vace_temp_folder, exist_ok=True)
                os.makedirs(mask_temp_folder, exist_ok=True)
                
                # Save sampled frames to temporary folders
                for i, (vace_frame, mask_frame) in enumerate(zip(vace_frames, mask_frames)):
                    vace_frame.save(os.path.join(vace_temp_folder, f"{i:04d}.png"))
                    mask_frame.save(os.path.join(mask_temp_folder, f"{i:04d}.png"))
                
                # Create new VideoData objects from sampled frames
                vace_video = VideoData(image_folder=vace_temp_folder, height=480, width=832)
                vace_video_mask = VideoData(image_folder=mask_temp_folder, height=480, width=832)
                
                print(f"âœ“ Reduced to {len(vace_video)} frames")
                
                # Clean up function (will be called after generation)
                def cleanup_temp_files():
                    if os.path.exists(temp_dir):
                        shutil.rmtree(temp_dir)
                        print("âœ“ Cleaned up temporary files")
            else:
                print(f"Video already has {original_frames} frames (â‰¤ {target_frames}), no reduction needed")
                def cleanup_temp_files():
                    pass  # No cleanup needed
        else:
            print("âš ï¸ No VACE video files found for this episode, skipping video generation...")
            continue

        print("\n=== Starting Enhanced Video Generation ===")
        
        # Generate enhanced video with both VACE and VACE-E
        video = pipe(
            prompt=robot_task_prompt,  # Use the same task-specific prompt
            negative_prompt="è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°",
            
            # Standard VACE parameters
            vace_video=vace_video,
            vace_video_mask=vace_video_mask,
            vace_reference_image=Image.open(episode_data['end_effector_image_path']).resize((832, 480)),
            vace_scale=1.0,
            
            # VACE-E Enhanced parameters (task-embodiment fusion)
            vace_e_text_features=vace_e_text_features,
            vace_e_hand_motion_sequence=vace_e_hand_motion_sequence,
            vace_e_object_trajectory_sequence=vace_e_object_trajectory_sequence,
            vace_e_object_ids=vace_e_object_ids,
            vace_e_text_mask=vace_e_text_mask,
            vace_e_motion_mask=vace_e_motion_mask,
            vace_e_trajectory_mask=vace_e_trajectory_mask,
            vace_e_embodiment_image_features=vace_e_embodiment_image_features,
            vace_e_scale=args.vace_e_scale,  # VACE-E conditioning strength
            
            # Generation parameters
            seed=1, 
            tiled=True,
            height=480,
            width=832,
            num_frames=len(vace_video)  # Use the reduced frame count (81 or less)
        )
        
        # Save episode-specific output
        # output_filename = f"./output/videos/{episode_data['task_name']}_{episode_data['episode_name']}.mp4"
        output_filename = f"./output/{args.output_dir}/{episode_data['task_name']}_{episode_data['episode_name']}.mp4"
        save_video(video, output_filename, fps=15, quality=5)
        print(f"âœ… Saved enhanced video: {output_filename}")

        # Clean up temporary files
        cleanup_temp_files()

        # Summary of generation for this episode
        print(f"\n=== Episode {episode_data['episode_name']} Generation Summary ===")
        print(f"ğŸ“¹ Output: {output_filename}")
        print(f"ğŸ¬ Frames: {len(vace_video)} frames")
        print(f"ğŸ“ Task prompt: {robot_task_prompt}")

        print("\nğŸ”§ VACE-E Features Used:")
        features_used = []
        if vace_e_text_features is not None:
            features_used.append("âœ“ Text task description")
        if vace_e_hand_motion_sequence is not None:
            features_used.append(f"âœ“ Hand motion sequence ({vace_e_hand_motion_sequence.shape[1]} timesteps)")
        if vace_e_object_trajectory_sequence is not None:
            features_used.append(f"âœ“ Object trajectories ({vace_e_object_trajectory_sequence.shape[2]} objects)")
        if vace_e_embodiment_image_features is not None:
            features_used.append("âœ“ End-effector embodiment image")

        if features_used:
            for feature in features_used:
                print(f"   {feature}")
            print(f"\nğŸ¤– Task-embodiment fusion successfully applied!")
            print(f"   VACE scale: 1.0 (standard video editing)")
            print(f"   VACE-E scale: {args.vace_e_scale} (task-embodiment guidance)")
        else:
            print("   âš ï¸ No VACE-E features loaded - using standard VACE only")

        print(f"\nğŸ¯ Enhanced robot manipulation video generation complete for {episode_data['episode_name']}!")
        # break
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ ALL EPISODES PROCESSED SUCCESSFULLY!")
    print(f"Generated {len([ep for ep in robot_episodes if ep['vace_video_path']])} enhanced robot manipulation videos")
    print(f"Each video combines traditional VACE editing with VACE-E task-embodiment fusion")
    print(f"for more precise and contextually aware robot manipulation video synthesis.")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
